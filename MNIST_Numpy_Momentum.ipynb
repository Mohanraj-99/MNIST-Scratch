{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_Numpy_Momentum.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClxIE7sVLYEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modules\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrTaLwCALeDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load MNIST Data\n",
        "mnist = fetch_openml('mnist_784')\n",
        "X, y = mnist[\"data\"], mnist[\"target\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltFYoKYwLj9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data - PreProcessing\n",
        "# Feature Scaling of X(Independant) data\n",
        "X = X / 255.0\n",
        "\n",
        "# One Hot Encoding of Y(dependant) data\n",
        "total = y.shape[0]\n",
        "y = y.reshape(1, total)\n",
        "Y_new = np.eye(10)[y.astype('int32')]\n",
        "Y = Y_new.T.reshape(10, total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTiXqIPIL-js",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data splitted into train and test datasets\n",
        "ntd = 60000\n",
        "X_train, X_test = X[:ntd].T, X[ntd:].T\n",
        "Y_train, Y_test = Y[:,:ntd], Y[:,ntd:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghFxhktwPnrI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper Parameters\n",
        "n_x = 784\n",
        "n_hn = 64\n",
        "lr = 4\n",
        "batch_size = 128\n",
        "beta = 0.7\n",
        "batches = ntd // batch_size\n",
        "epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MKyctbTPYJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Weights & Bias Initialization  (He Initialization)\n",
        "init =  { \n",
        "           \"W1\": np.random.randn(n_hn, n_x) * np.sqrt(1. / n_x),\n",
        "           \"b1\": np.zeros((n_hn, 1)) * np.sqrt(1. / n_x),\n",
        "           \"W2\": np.random.randn(10, n_hn) * np.sqrt(1. / n_hn),\n",
        "           \"b2\": np.zeros((10, 1)) * np.sqrt(1. / n_hn) \n",
        "        }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpv7-TqDbAnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimizer init (Momentum)\n",
        "V_dW1 = np.zeros(init[\"W1\"].shape)\n",
        "V_db1 = np.zeros(init[\"b1\"].shape)\n",
        "V_dW2 = np.zeros(init[\"W2\"].shape)\n",
        "V_db2 = np.zeros(init[\"b2\"].shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA6FFjZWOC5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Activation func , Loss func , Forward pass , Backward pass\n",
        "def sigmoid(z):\n",
        "    return 1. / (1. + np.exp(-z))\n",
        "\n",
        "def compute_loss(Y, Y_hat):\n",
        "    l = np.sum(Y * (np.log(Y_hat)))\n",
        "    return -(1./Y.shape[1]) * l\n",
        "\n",
        "def feed_forward(X, init):\n",
        "    Z1 = np.matmul(init[\"W1\"], X) + init[\"b1\"]\n",
        "    A1 = sigmoid(Z1)\n",
        "    Z2 = np.matmul(init[\"W2\"], A1) + init[\"b2\"]\n",
        "    A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)\n",
        "    dic = {\"Z1\":Z1, \"A1\":A1, \"Z2\":Z2, \"A2\":A2}\n",
        "    return dic\n",
        "\n",
        "def back_propagate(X, Y, init, dic):\n",
        "    dZ2 = dic[\"A2\"] - Y\n",
        "    dW2 = (1./batch_size) * np.matmul(dZ2, dic[\"A1\"].T)\n",
        "    db2 = (1./batch_size) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "    dA1 = np.matmul(init[\"W2\"].T, dZ2)\n",
        "    dZ1 = dA1 * sigmoid(dic[\"Z1\"]) * (1 - sigmoid(dic[\"Z1\"]))\n",
        "    dW1 = (1./batch_size) * np.matmul(dZ1, X.T)\n",
        "    db1 = (1./batch_size) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9W3EDERRhEz",
        "colab_type": "code",
        "outputId": "cbf48cfd-8fdb-472f-dc6d-22d998439ab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "# train\n",
        "for i in range(epochs):\n",
        "\n",
        "    new_index = np.random.permutation(ntd)\n",
        "    X_train, Y_train = X_train[:, new_index], Y_train[:, new_index]\n",
        "    for j in range(batches):\n",
        "\n",
        "        begin = j * batch_size\n",
        "        end = begin + batch_size\n",
        "        X = X_train[:, begin:end]\n",
        "        Y = Y_train[:, begin:end]\n",
        "        dic = feed_forward(X, init)\n",
        "        grads = back_propagate(X, Y, init, dic)\n",
        "\n",
        "        V_dW1 = (beta * V_dW1 + (1. - beta) * grads[\"dW1\"])\n",
        "        V_db1 = (beta * V_db1 + (1. - beta) * grads[\"db1\"])\n",
        "        V_dW2 = (beta * V_dW2 + (1. - beta) * grads[\"dW2\"])\n",
        "        V_db2 = (beta * V_db2 + (1. - beta) * grads[\"db2\"])\n",
        "\n",
        "        init[\"W1\"] -= lr * V_dW1\n",
        "        init[\"b1\"] -= lr * V_db1\n",
        "        init[\"W2\"] -= lr * V_dW2\n",
        "        init[\"b2\"] -= lr * V_db2\n",
        "\n",
        "    dic = feed_forward(X_train, init)\n",
        "    train_cost = compute_loss(Y_train, dic[\"A2\"])\n",
        "    dic = feed_forward(X_test, init)\n",
        "    test_cost = compute_loss(Y_test, dic[\"A2\"])\n",
        "    print(\"Epoch {}: training cost = {}, test cost = {}\".format(i+1 ,train_cost, test_cost))\n",
        "\n",
        "print(\"Report :\")\n",
        "\n",
        "dic = feed_forward(X_test, init)\n",
        "predictions = np.argmax(dic[\"A2\"], axis=0)\n",
        "labels = np.argmax(Y_test, axis=0)\n",
        "\n",
        "print(classification_report(predictions, labels))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: training cost = 0.14695942976786477, test cost = 0.15316682827608508\n",
            "Epoch 2: training cost = 0.09810110254813233, test cost = 0.11409289504490983\n",
            "Epoch 3: training cost = 0.07479689385473891, test cost = 0.10193870409883977\n",
            "Epoch 4: training cost = 0.06255561093864297, test cost = 0.09909269296085231\n",
            "Epoch 5: training cost = 0.048807986600367, test cost = 0.08378839566323108\n",
            "Epoch 6: training cost = 0.04349833769877289, test cost = 0.08887109101472838\n",
            "Epoch 7: training cost = 0.04004782368960049, test cost = 0.08627839113437202\n",
            "Epoch 8: training cost = 0.03008310106709013, test cost = 0.0844433034352861\n",
            "Epoch 9: training cost = 0.03186298477064734, test cost = 0.08852843931284714\n",
            "Epoch 10: training cost = 0.023971687075653766, test cost = 0.08781837792855123\n",
            "Report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.97      0.98       990\n",
            "           1       0.99      0.99      0.99      1130\n",
            "           2       0.96      0.98      0.97      1017\n",
            "           3       0.98      0.96      0.97      1027\n",
            "           4       0.97      0.98      0.97       968\n",
            "           5       0.97      0.96      0.96       904\n",
            "           6       0.97      0.97      0.97       961\n",
            "           7       0.97      0.97      0.97      1032\n",
            "           8       0.97      0.97      0.97       969\n",
            "           9       0.96      0.97      0.96      1002\n",
            "\n",
            "    accuracy                           0.97     10000\n",
            "   macro avg       0.97      0.97      0.97     10000\n",
            "weighted avg       0.97      0.97      0.97     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}